{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c0c111-70bd-49f3-8765-a271e98efc83",
   "metadata": {},
   "source": [
    "# 功能说明\n",
    "\n",
    "- 读取文件'V:\\20240920\\all_in_one_1\\2_processed_data_BERT.csv'\n",
    "- 把'lette'列的文本逐一送给指定模型进行判断\n",
    "- 将模型判断的结果写入新加的特定列中\n",
    "- 将源文件的内容和模型得到的结果写入文件'V:\\20240920\\way2\\way2_base_data_add_emotion.csv'中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea10873-9487-4e3b-a6fc-7cd3d2a3ab93",
   "metadata": {},
   "source": [
    "## 运行环境设置\r\n",
    "\r\n",
    "先下载模型文件，以便离线使用\r\n",
    "\r\n",
    "**powershell**    \r\n",
    "\r\n",
    "- $env:HF_HUB_OFFLINE=\"0\"\r\n",
    "- $env:HF_DATASETS_OFFLINE=\"0\"\r\n",
    "- $env:HF_ENDPOINT = \"https://hf-mirror.com\"   \r\n",
    "- huggingface-cli download --resume-download --repo-type model SamLowe/roberta-base-go_emotions --local-dir \"V:/huggingface/model/SamLowe/roberta-base-go_emotions   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750887d-a360-42f6-8cb7-797d42e30e6f",
   "metadata": {},
   "source": [
    "## 导入需要的package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a035d071-5634-409c-a96a-c802c4e7451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb717138-18b8-4a1b-971a-1da92f87ec3c",
   "metadata": {},
   "source": [
    "# code all-in-one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa9b8f-3d8f-4d8a-8f88-15c2e1f889e1",
   "metadata": {},
   "source": [
    "## SamLowe/roberta-base-go_emotions \n",
    "\n",
    "<font color=red size=3> 这个模型最多只支持512个输入tokens，否则报错，为此pipeline()需要增加参数 truncation=True 来截断输入的超长文本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f60021-8e56-40ce-bd31-e78046631585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----文件 V:\\20240920\\all_in_one_1\\2_processed_data_BERT.csv 的编码检测结果是: utf-8\n",
      "以编码检测的结果 utf-8 读取文件成功！\n",
      "\n",
      "Data statistics:\n",
      "Total number of texts: 5583\n",
      "Number of None values: 0\n",
      "Number of valid texts after removing None: 5583\n",
      "\n",
      " Data statistics after Remove rows with None values :\n",
      " Total number of texts: 5583\n",
      " Number of None values: 0\n",
      "\n",
      "---columns name list of df_result is: ----\n",
      " ['index', 'Unnamed: 0', 'Coder1', 'Coder2', 'ResponseID', 'NA_column', 'letter', 'number_of_characters', 'coding1', 'coding2', 'coding3', 'Mostmoderntheoriesofdecisionmakingrecognizethefactthatdecisionsd', 'EFFORT', 'Age', 'DScores', 'IATTaken', 'ReasonifMissing', 'OUT', 'Gender', 'WhatisyourgenderidentityOtherpleasespecifyText', 'RACE_all', 'Race_primary', 'RACE_FINAL', 'RACE_COMBINEASIAN', 'RACE_OUT', 'Inwhatcountrywasyourmotherborn', 'Inwhatcountrywasyourfatherborn', 'Listthecountrieswhereyourgrandparentswereborn', 'ChooseOneRaceOtherpeoplebelievemorethanonetermdescribesthem.Ifth', 'POLITICS_scale', 'Politics', 'PoliticsForce', 'PoliticsFINAL', 'unsual', 'typical', 'Iwasnotawareoftheinformationpresentedinthearticle', 'Ifoundthecontentofthearticlesurprising', 'Ithinkthestatisticsreportedinthearticleareaccurateandtrue', 'ThisarticlefocusedonwomensSTRENGTHSasleaders', 'ThisarticlefocusedonwomensWEAKNESSESasleaders', 'ThisarticlefocusedontheOBSTACLESwomenfaceasleaders', 'ThisarticlefocusedonwhatwomenhaveOVERCOMEtolead', 'important', 'wantincrease', 'powertochange', 'abletochange', 'workforman', 'voteforwoman', 'womenbetterleaders', 'manincharge', 'workforfemaleCEO', 'Womenholdingpositionsofpowerbenefitmycommunitystateandcountry', 'advantagestowomen', 'morefemaleleadersinanorganizationIbelievetheteamismoreproductive', 'Genderbiaswomenfacewhentheyareupforelectionorpromotion', 'Unearnedprivilegegrantedtomenbecauseoftheirmaleidentity', 'Meninpowerhelpingothermensucceedatthecostofoverlookingwomen', 'Lackofwomenapplyingforleadershippositionsinthefirstplace', 'Womennaturallynotinterestedinbeinginpositionsofpower', 'Lackofqualifiedwomentofillleadershippositions', 'Men‚Äôsgreaternaturalabilitytobeleadersthanwomen', 'Women‚Äôsunwillingnesstoassertthemselves', 'Menandwomennaturallyhavedifferentabilities', 'Differencesbetweenwomenandmen‚ÄôspersonalitiesareintheirDNA', 'Maleandfemalebrainsprobablyworkinverydifferentways', 'Womenareinnatelymorenurturingthanmen', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFacebo', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_A', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_B', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_C', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_D', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_E', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_F', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_G', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_H', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_I', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_J', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_K', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_L', 'HowwillingwouldyoubetoposteachofthefollowinghashtagsonyourFace_M', 'Howwillingwouldyoubeto‚Ä¶ChangeyourFacebookInstagramorTwitte', 'Howwillingwouldyoubeto‚Ä¶PostapubliccommentonFacebookorTwitt', 'Howwillingwouldyoubeto‚Ä¶Writeashortlettertotheeditorofanews', 'Howwillingwouldyoubeto‚Ä¶Participateinanonlinegroupchattohel', 'Howwillingwouldyoubeto‚Ä¶Participateinapeacefulmarchtocelebr', 'Howwillingwouldyoubeto‚Ä¶Emailfriendsandfamilymembersthearti', 'Howwillingwouldyoubeto‚Ä¶Supportgenderequityintheworkplace', 'Prime', 'DOMAIN', 'FRAME', 'UnusualMINUSTypical', 'TypicalR', 'AvgUnusualTypicalR', 'NotAwareSurprising', 'ImporantWant', 'PowerAble', 'Motivation', 'WillingAVG', 'WillingfirstsetAVG', 'Willingset2AVG', 'Essentialism', 'AGECATEGORY', 'AgeCat2', 'ZEFFORTOutliersRemoved', 'AgeCat3', 'AgeCat4', 'ZEffortOutliersReplaced', 'EffortOutliersReplaced', 'WriteALetterBINARY', 'VERYLIBERAL_LIBERAL', 'WorkforWomenAVG', 'WorkforMenAVG', 'AdvantagesFemaleLeadershipAVG', 'INTENTIONS_AVG', 'INTENTIONS2_AVG', 'DomainBI', 'FrameBI', 'DomainBI_FrameBI', 'RACE_MajMin', 'processed_text', 'punctuation_count', 'word_count', 'row_null_count', 'is_short', 'null_columns', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "当前没有可用的GPU设备\n",
      "使用CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Coder1</th>\n",
       "      <th>Coder2</th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>NA_column</th>\n",
       "      <th>letter</th>\n",
       "      <th>number_of_characters</th>\n",
       "      <th>coding1</th>\n",
       "      <th>coding2</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diya Basu</td>\n",
       "      <td>Alexandra Weiss</td>\n",
       "      <td>R_005dEWNt4jenOql</td>\n",
       "      <td>6108957f175a2eb650221b8e</td>\n",
       "      <td>Greeting Senator,\\n \\n  Women's Global Empower...</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.062789</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.296698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Anna Owens</td>\n",
       "      <td>Isha Gupta</td>\n",
       "      <td>R_00y8lNWUZCArLjP</td>\n",
       "      <td>A245CPNDUDHUYX</td>\n",
       "      <td>Hello:\\n \\n I feel that this bill should be re...</td>\n",
       "      <td>289.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.043968</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.040688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Ashley Kim</td>\n",
       "      <td>Quincy Lherisson</td>\n",
       "      <td>R_018hCVjTzrhwNQB</td>\n",
       "      <td>A1JM5XNB4NCZR6</td>\n",
       "      <td>Dear Senator\\n This bell is important for advo...</td>\n",
       "      <td>290.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.008293</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.067880</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.738031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0      Coder1            Coder2         ResponseID  \\\n",
       "0      0           0   Diya Basu   Alexandra Weiss  R_005dEWNt4jenOql   \n",
       "1      1           2  Anna Owens        Isha Gupta  R_00y8lNWUZCArLjP   \n",
       "2      2           3  Ashley Kim  Quincy Lherisson  R_018hCVjTzrhwNQB   \n",
       "\n",
       "                  NA_column  \\\n",
       "0  6108957f175a2eb650221b8e   \n",
       "1            A245CPNDUDHUYX   \n",
       "2            A1JM5XNB4NCZR6   \n",
       "\n",
       "                                              letter  number_of_characters  \\\n",
       "0  Greeting Senator,\\n \\n  Women's Global Empower...                 560.0   \n",
       "1  Hello:\\n \\n I feel that this bill should be re...                 289.0   \n",
       "2  Dear Senator\\n This bell is important for advo...                 290.0   \n",
       "\n",
       "  coding1 coding2  ...      love nervousness  optimism     pride  realization  \\\n",
       "0       1     1.0  ...  0.005607    0.000668  0.062789  0.004094     0.017382   \n",
       "1       1     1.0  ...  0.001567    0.000568  0.043968  0.001635     0.010425   \n",
       "2      -1    -1.0  ...  0.000758    0.000584  0.008293  0.000560     0.067880   \n",
       "\n",
       "     relief   remorse   sadness  surprise   neutral  \n",
       "0  0.002048  0.000658  0.001718  0.000797  0.296698  \n",
       "1  0.003761  0.003688  0.002551  0.000864  0.040688  \n",
       "2  0.001395  0.002252  0.005963  0.000527  0.738031  \n",
       "\n",
       "[3 rows x 153 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import chardet\n",
    "\n",
    "# 注意：文件 V:\\20240920\\all_in_one_1\\2_processed_data_BERT.csv 的编码检测结果是: Windows-1252\n",
    "input_file = r'V:\\20240920\\all_in_one_1\\2_processed_data_BERT.csv'\n",
    "\n",
    "# 检测文件编码\n",
    "with open(input_file, 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    file_encoding = result['encoding']\n",
    "\n",
    "print(f\"----文件 {input_file} 的编码检测结果是: {file_encoding}\")\n",
    "\n",
    "# 读取 CSV 文件\n",
    "try:\n",
    "    # header=1：表示将第二行作为列名（因为 Python 从 0 开始计数，header=1 表示第二行）\n",
    "    # skiprows=0：表示不跳过任何行。如果设置为 skiprows=1，则会跳过第一行。\n",
    "    df = pd.read_csv(input_file, encoding=file_encoding) \n",
    "    print(f\"以编码检测的结果 {file_encoding} 读取文件成功！\")\n",
    "    # print(df.head())  # 显示前 5 行数据\n",
    "except UnicodeDecodeError:\n",
    "    print(\"检测到的编码无法正确读取文件，尝试其他编码...\")\n",
    "    try:\n",
    "        # 尝试 gbk 编码\n",
    "        df = pd.read_csv(input_file, encoding='gbk')  # 第一行是无效数据，第二行是列名\n",
    "        print(\"以 GBK 编码读取文件成功！\")\n",
    "        # print(df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"文件读取失败，错误信息: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"文件读取失败，错误信息: {e}\")\n",
    "\n",
    "\n",
    "# show statistics of None values in letter column\n",
    "print(\"\\nData statistics:\")\n",
    "print(f\"Total number of texts: {len(df)}\")\n",
    "print(f\"Number of None values: {df['letter'].isnull().sum()}\")\n",
    "\n",
    "# Remove rows with None values\n",
    "# 必须删除 'letter' 列为空的行，否则 SentenceTransformer 会报错\n",
    "df_clean = df.dropna(subset=['letter']).reset_index()                # must reset_index\n",
    "print(f\"Number of valid texts after removing None: {len(df_clean)}\")\n",
    "\n",
    "print(\"\\n Data statistics after Remove rows with None values :\")\n",
    "print(f\" Total number of texts: {len(df_clean)}\")\n",
    "print(f\" Number of None values: {df_clean['letter'].isnull().sum()}\")\n",
    "\n",
    "# 定义新加列的列名列表\n",
    "columns = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
    "           'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement',\n",
    "           'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "           'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "\n",
    "# 创建新的的DataFrame\n",
    "df_result = df_clean.copy(deep=True)\n",
    "\n",
    "# 通过为新的列赋值来加入新列\n",
    "for col in columns:\n",
    "    df_result[col] = np.nan\n",
    "\n",
    "print(f\"\\n---columns name list of df_result is: ----\\n {df_result.columns.tolist()}\")\n",
    "\n",
    "# set compute device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"当前GPU设备ID有:\", torch.cuda.current_device())\n",
    "    device_id = torch.cuda.current_device() # 指定使用的GPU设备ID\n",
    "    print(\"使用的GPU设备ID:\", device_id)\n",
    "else:\n",
    "    print(\"当前没有可用的GPU设备\")\n",
    "    device_id = \"cpu\"     # 明确指定使用CPU设备\n",
    "    print(\"使用CPU\")\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", \n",
    "                      model=r\"V:\\huggingface\\model\\SamLowe\\roberta-base-go_emotions\", \n",
    "                      top_k=None, \n",
    "                      device=device_id,\n",
    "                      truncation=True)   # important !!!\n",
    "\n",
    "# 为 df_result 的各新加列填充对应的emotion值\n",
    "for i in range(len(df_result['letter'])):\n",
    "    letter_i = df_result.loc[i,'letter']\n",
    "    model_outputs = classifier(letter_i)\n",
    "    data_dict = {item['label']: item['score'] for item in  model_outputs[0]} #将model_outputs[0]转换为以标签为键，分数为值的字典，方便后续根据标签快速查询分数\n",
    "    df_result.loc[i,'admiration'] = data_dict['admiration']\n",
    "    df_result.loc[i,'amusement'] = data_dict['amusement']\n",
    "    df_result.loc[i,'anger'] = data_dict['anger']\n",
    "    df_result.loc[i,'annoyance'] = data_dict['annoyance']\n",
    "    df_result.loc[i,'approval'] = data_dict['approval']\n",
    "    df_result.loc[i,'caring'] = data_dict['caring']\n",
    "    df_result.loc[i,'confusion'] = data_dict['confusion']\n",
    "    df_result.loc[i,'curiosity'] = data_dict['curiosity']\n",
    "    df_result.loc[i,'desire'] = data_dict['desire']\n",
    "    df_result.loc[i,'disappointment'] = data_dict['disappointment']\n",
    "    df_result.loc[i,'disapproval'] = data_dict['disapproval']\n",
    "    df_result.loc[i,'disgust'] = data_dict['disgust']\n",
    "    df_result.loc[i,'embarrassment'] = data_dict['embarrassment']\n",
    "    df_result.loc[i,'excitement'] = data_dict['excitement']\n",
    "    df_result.loc[i,'fear'] = data_dict['fear']\n",
    "    df_result.loc[i,'gratitude'] = data_dict['gratitude']\n",
    "    df_result.loc[i,'grief'] = data_dict['grief']\n",
    "    df_result.loc[i,'joy'] = data_dict['joy']\n",
    "    df_result.loc[i,'love'] = data_dict['love']\n",
    "    df_result.loc[i,'nervousness'] = data_dict['nervousness']\n",
    "    df_result.loc[i,'optimism'] = data_dict['optimism']\n",
    "    df_result.loc[i,'pride'] = data_dict['pride']\n",
    "    df_result.loc[i,'realization'] = data_dict['realization']\n",
    "    df_result.loc[i,'relief'] = data_dict['relief']\n",
    "    df_result.loc[i,'remorse'] = data_dict['remorse']\n",
    "    df_result.loc[i,'sadness'] = data_dict['sadness']\n",
    "    df_result.loc[i,'surprise'] = data_dict['surprise']\n",
    "    df_result.loc[i,'neutral'] = data_dict['neutral']\n",
    "    \n",
    "\n",
    "# 保存结果到文件\n",
    "df_result.to_csv(r'V:\\20240920\\all_in_one_1\\3_add_emotion.csv', index=False)\n",
    "df_result.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c954330-21ec-4b86-bf7e-241aa4d93d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaebead-b9d6-4934-b524-cfa467cf3198",
   "metadata": {},
   "source": [
    "# test code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
