{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17437819-8457-415a-bc83-22817d97df20",
   "metadata": {},
   "source": [
    "# 版本变更说明\n",
    "**enbedding 段的 input_file 是《way1\\1_merged table v1.xlsx》**，它有如下特点：  \n",
    "\n",
    "- 来路明确，是我用《way1\\1_merge_table.ipynb》脚本合并 WTOS 和 Data power 文件而成\n",
    "\n",
    "**enbedding 段增加了矫正乱码的代码**，为方便检查，我把原始letter列保存在 orginal_letter 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f673556-df4e-4f6c-ba39-5a16f8e954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478595c-f058-440e-92eb-14ae077e4349",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b003189c-b3ad-4449-917f-06ec53391f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Data statistics:\n",
      "Total number of texts: 8845\n",
      "Number of None values: 3262\n",
      "Number of valid texts after removing None: 5583\n",
      "Creating embeddings...\n",
      "\n",
      "Saving embeddings...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5583, 768)\n",
      "length of csv_output_file: 5583\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"V:\\20240920\\way1\\1_merged table v1.xlsx\"\n",
    "\n",
    "csv_output_file = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "npy_output_file = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=\"sheet1\")\n",
    "df['orginal_letter'] = df['letter']\n",
    "\n",
    "## replace the specific mistake characters\n",
    "#详见《 letter列文本中的乱码及处理方法.xlsx》 \n",
    "replacements = {\n",
    "            '‚Äô': \"'\",\n",
    "            '‚äô': \"'\",\n",
    "            '√©': \" \",\n",
    "            'Äì¬†': \" \",\n",
    "            '‚Äú': \"\\\"\",\n",
    "            '‚Äù': \"\\'\",\n",
    "            '‚Äî': \"--\",\n",
    "            '‚Ä¶': \" \",\n",
    "            '√≥': \"o\"\n",
    "        }\n",
    "for old, new in replacements.items():\n",
    "    df['letter'] = df['letter'].str.replace(old, new, regex=False)\n",
    "\n",
    "\n",
    "# Remove None values and show statistics\n",
    "print(\"\\nData statistics:\")\n",
    "print(f\"Total number of texts: {len(df)}\")\n",
    "print(f\"Number of None values: {df['letter'].isnull().sum()}\")\n",
    "   \n",
    "# Remove rows with None values\n",
    "# 必须删除 'letter' 列为空的行，否则 SentenceTransformer 会报错\n",
    "df_clean = df.dropna(subset=['letter'])\n",
    "print(f\"Number of valid texts after removing None: {len(df_clean)}\")\n",
    "\n",
    "# Creating embeddings\n",
    "print(\"Creating embeddings...\")\n",
    "texts = df_clean['letter'].tolist()\n",
    "# texts = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer(r\"V:\\huggingface\\model\\sentence-transformers/all-mpnet-base-v2\") # off-line mode\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') # on-line mode\n",
    "\n",
    "embeddings = model.encode(texts, max_length=2048)\n",
    "\n",
    "# Saving embeddings  \n",
    "print(\"\\nSaving embeddings...\")\n",
    "# np_embeddings = embeddings.numpy()\n",
    "np.save(npy_output_file, embeddings)\n",
    "\n",
    "# compare length of embedding and data file \n",
    "print(\"\\n --- compare length of embedding and data file ---\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"length of csv_output_file: {len(df_clean['letter'])}\")\n",
    "\n",
    "# Save clean data for reference\n",
    "df_clean.to_csv(csv_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a6e02-ccae-43a3-b9b1-872896d50a80",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f72d6-9a45-4333-bc5d-06a4ca732e0b",
   "metadata": {},
   "source": [
    "## explain variance percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c000fc28-5ed3-47df-a8e0-75d89db77528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 35 components to explain 70.0% variance.\n",
      "Need 65 components to explain 80.0% variance.\n",
      "Need 138 components to explain 90.0% variance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def components_for_variance(embeddings_path, n):\n",
    "    embed = np.load(embeddings_path)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embed)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_embeddings)\n",
    "    \n",
    "    # Calculate cumulative variance ratio\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    num_components = np.argmax(cumulative_variance >= n) + 1  \n",
    "    \n",
    "    print(f\"Need {num_components} components to explain {n * 100}% variance.\")\n",
    "    return num_components\n",
    "\n",
    "\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "components_for_variance(embeddings_path, 0.7)\n",
    "components_for_variance(embeddings_path, 0.8)\n",
    "components_for_variance(embeddings_path, 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd9cb2-45bb-4196-9c42-e87f81e94287",
   "metadata": {},
   "source": [
    "## Themes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93677bab-48d1-4b52-b049-d42fd47df4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 35 components to explain 70.0% variance.\n",
      "\n",
      " --- need 35 components for explained variance ratio 0.7 ---\n",
      "\n",
      "Loading data...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5583, 768)\n",
      "length of data file: 5583\n",
      "\n",
      "--- Standardizing embeddings...\n",
      "\n",
      "--- Applying PCA with 35 components...\n",
      "\n",
      " --- Total explained variance: 0.7012\n",
      "\n",
      "--- Explained Variance Ratio per Component ---\n",
      "Component 1: 0.1087\n",
      "Component 2: 0.0813\n",
      "Component 3: 0.0642\n",
      "Component 4: 0.0500\n",
      "Component 5: 0.0372\n",
      "Component 6: 0.0288\n",
      "Component 7: 0.0268\n",
      "Component 8: 0.0235\n",
      "Component 9: 0.0224\n",
      "Component 10: 0.0198\n",
      "Component 11: 0.0191\n",
      "Component 12: 0.0167\n",
      "Component 13: 0.0153\n",
      "Component 14: 0.0151\n",
      "Component 15: 0.0134\n",
      "Component 16: 0.0129\n",
      "Component 17: 0.0115\n",
      "Component 18: 0.0113\n",
      "Component 19: 0.0101\n",
      "Component 20: 0.0100\n",
      "Component 21: 0.0093\n",
      "Component 22: 0.0092\n",
      "Component 23: 0.0085\n",
      "Component 24: 0.0081\n",
      "Component 25: 0.0078\n",
      "Component 26: 0.0075\n",
      "Component 27: 0.0071\n",
      "Component 28: 0.0068\n",
      "Component 29: 0.0065\n",
      "Component 30: 0.0060\n",
      "Component 31: 0.0056\n",
      "Component 32: 0.0055\n",
      "Component 33: 0.0054\n",
      "Component 34: 0.0052\n",
      "Component 35: 0.0050\n",
      "\n",
      "\n",
      " --- Analyzing themes in each component...\n",
      "old File 'V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# select explained variance ratio\n",
    "explained_variance_ratio = 0.7\n",
    "number_components = components_for_variance(embeddings_path, explained_variance_ratio)\n",
    "print(f\"\\n --- need {number_components} components for explained variance ratio {explained_variance_ratio} ---\\n\")\n",
    "\n",
    "def analyze_pca_themes(embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\", \n",
    "                       data_path=r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\",\n",
    "                       n_components = 34,   \n",
    "                       n_top_texts=5):\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # compare length of embedding and data file \n",
    "    print(\"\\n --- compare length of embedding and data file ---\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"length of data file: {len(df['letter'])}\")\n",
    "    \n",
    "    # Check data length consistency\n",
    "    if embeddings.shape[0] != len(df):\n",
    "        raise ValueError(\"The number of embeddings does not match the number of texts in the data file.\")\n",
    "  \n",
    "    # Standardize embeddings\n",
    "    print(\"\\n--- Standardizing embeddings...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "   \n",
    "    # Apply PCA\n",
    "    print(f\"\\n--- Applying PCA with {n_components} components...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(scaled_embeddings)\n",
    "   \n",
    "    print(f\"\\n --- Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    # Print explained variance ratio per component\n",
    "    print(\"\\n--- Explained Variance Ratio per Component ---\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"Component {i+1}: {ratio:.4f}\")\n",
    "   \n",
    "    # Analyze themes\n",
    "    print(\"\\n\\n --- Analyzing themes in each component...\")\n",
    "    themes = {}\n",
    "    for i in range(n_components):\n",
    "        # Get component scores\n",
    "        scores = pca_result[:, i]\n",
    "        \n",
    "        # Get top and bottom texts\n",
    "        top_indices = np.argsort(scores)[-n_top_texts:]\n",
    "        bottom_indices = np.argsort(scores)[:n_top_texts]\n",
    "        \n",
    "        # Add statistics\n",
    "        themes[f'Component_{i+1}'] = {\n",
    "            'positive': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in reversed(top_indices)],\n",
    "            'negative': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in bottom_indices],\n",
    "            'score_stats': {\n",
    "                'mean': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "        }\n",
    "   \n",
    "    return pca_result, themes\n",
    "\n",
    "def save_themes_to_excel(themes, output_path):\n",
    "    \"\"\"\n",
    "    Save the thematic analysis results to an Excel file.\n",
    "    \"\"\"\n",
    "    # Step 1: Check if the file exists and delete it\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"old File '{output_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File '{output_path}' does not exist. Creating a new file.\")\n",
    "    \n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "        for component, theme_data in themes.items():\n",
    "            # Create a DataFrame for positive examples\n",
    "            positive_df = pd.DataFrame(theme_data['positive'], columns=['Text', 'Score'])\n",
    "            positive_df.to_excel(writer, sheet_name=f\"{component}_Positive\", index=False)\n",
    "            \n",
    "            # Create a DataFrame for negative examples\n",
    "            negative_df = pd.DataFrame(theme_data['negative'], columns=['Text', 'Score'])\n",
    "            negative_df.to_excel(writer, sheet_name=f\"{component}_Negative\", index=False)\n",
    "            \n",
    "            # Add statistics to the Excel file\n",
    "            stats_df = pd.DataFrame([theme_data['score_stats']])\n",
    "            stats_df.to_excel(writer, sheet_name=f\"{component}_Stats\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Apply PCA and analyze themes\n",
    "    pca_result, themes = analyze_pca_themes(n_components = number_components)\n",
    "    \n",
    "    # Save PCA results\n",
    "    np.save(r\"V:\\20240920\\theme_analysis_act3301\\pca_results.npy\", pca_result)\n",
    "    \n",
    "    # Save themes to Excel file\n",
    "    save_themes_to_excel(themes, r\"V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b976fc-8a25-4fe0-b395-3d3d75fbde62",
   "metadata": {},
   "source": [
    "# Varimax Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18eaaea2-92f8-4b09-8916-47d11dd92cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "\n",
      "---Loadings:\n",
      " [[-0.05831511  0.01672853 -0.02240267 ... -0.01173237  0.0099031\n",
      "   0.0402006 ]\n",
      " [-0.03672548  0.02988562  0.04441902 ... -0.00957623  0.01379264\n",
      "   0.03293107]\n",
      " [ 0.03683105 -0.03172079 -0.00223721 ...  0.00243738  0.04210471\n",
      "  -0.02097005]\n",
      " ...\n",
      " [-0.05399645 -0.04809344 -0.02057458 ...  0.01916452  0.06832749\n",
      "  -0.02811958]\n",
      " [-0.0551717  -0.03301169  0.05814507 ...  0.00180005  0.0167277\n",
      "   0.04576058]\n",
      " [-0.01073436 -0.01206578 -0.02260295 ... -0.03737618  0.03881303\n",
      "   0.01323039]]\n",
      "(768, 35)\n",
      "(35, 768)\n",
      "\n",
      "---Rotated Loadings:\n",
      " [[-0.05929761  0.03455264 -0.02050486 ... -0.03448478  0.01289598\n",
      "   0.04559542]\n",
      " [ 0.0179012  -0.02260267  0.00486097 ... -0.012991    0.03041592\n",
      "  -0.00812172]\n",
      " [-0.01424091  0.02183586  0.00840591 ...  0.02779863  0.00460952\n",
      "  -0.02039335]\n",
      " ...\n",
      " [-0.0695141  -0.01609712 -0.03669699 ... -0.06024928  0.00341119\n",
      "  -0.04632895]\n",
      " [-0.04060106 -0.05536942  0.00582074 ...  0.00981039 -0.027425\n",
      "   0.00832968]\n",
      " [-0.01053765 -0.03033187  0.0006533  ...  0.01481464  0.00891599\n",
      "  -0.00709886]]\n",
      "Rotated Loadings's shape: (768, 35)\n",
      "(35, 768)\n",
      "(768, 35)\n"
     ]
    }
   ],
   "source": [
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# load data\n",
    "embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# Standardize embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# 70% varianc explanation.\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis \n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# transposes the matrix so that each row corresponds to a feature and each column corresponds to a component.\n",
    "loadings = pca.components_.T  \n",
    "print(\"\\n---Loadings:\\n\", loadings)\n",
    "print(loadings.shape)\n",
    "print(loadings.T.shape)\n",
    "\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "print(\"\\n---Rotated Loadings:\\n\", rotated_loadings)\n",
    "print(f\"Rotated Loadings's shape: {rotated_loadings.shape}\")\n",
    "print(pca.components_.shape)\n",
    "print(pca.components_.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc525d-2a69-4d70-8c92-539b38afc037",
   "metadata": {},
   "source": [
    "## get the new score after varimax rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbaea812-f6ec-48b9-8845-f331f6e2ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "Rotated Scores' shape: (5583, 35)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# Step 1: Load the Data\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "data_path = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "\n",
    "embeddings = np.load(embeddings_path)\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Standardize the Embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Step 3: Perform PCA and Varimax Rotation\n",
    "# Perform PCA to determine the number of components explaining 90% variance\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis\n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "# Fit PCA with the selected number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# Extract PCA loadings and transpose them\n",
    "loadings = pca.components_.T\n",
    "\n",
    "# Apply Varimax rotation\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "# Step 4: Project Data onto Rotated Components\n",
    "# Calculate the new scores by multiplying the standardized embeddings with the rotated loadings\n",
    "rotated_scores = np.dot(scaled_embeddings, rotated_loadings)\n",
    "\n",
    "# Step 5: Save the New Scores\n",
    "# Save the rotated scores to a new file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores.npy\"\n",
    "np.save(output_path, rotated_scores)\n",
    "\n",
    "# Print the shape of the rotated scores\n",
    "print(f\"Rotated Scores' shape: {rotated_scores.shape}\")\n",
    "\n",
    "# Optional: Save the rotated scores with corresponding text data to a CSV file\n",
    "rotated_scores_df = pd.DataFrame(rotated_scores, columns=[f\"Rotated_Component_{i+1}\" for i in range(n_components)])\n",
    "rotated_scores_df[\"letter\"] = df[\"letter\"].values\n",
    "rotated_scores_df.to_csv(r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830b8fd-bbc5-441d-be46-9ab2e3bcc4ad",
   "metadata": {},
   "source": [
    "## get top_n positive and negative score letter of each Rotated Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd44a27a-f182-4038-b2a8-dc34b0351c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Component      Type  \\\n",
      "0     Rotated_Component_1  Positive   \n",
      "1     Rotated_Component_1  Positive   \n",
      "2     Rotated_Component_1  Positive   \n",
      "3     Rotated_Component_1  Positive   \n",
      "4     Rotated_Component_1  Positive   \n",
      "..                    ...       ...   \n",
      "345  Rotated_Component_35  Negative   \n",
      "346  Rotated_Component_35  Negative   \n",
      "347  Rotated_Component_35  Negative   \n",
      "348  Rotated_Component_35  Negative   \n",
      "349  Rotated_Component_35  Negative   \n",
      "\n",
      "                                                Letter      Score  \n",
      "0    I do not support the reintroduction of the bil...  13.588921  \n",
      "1    I don't believe bill S. 3301 should be reintro...  11.836024  \n",
      "2    Please don't reintroduce the Women's Global Em...  10.828992  \n",
      "3    I feel as though we should not re-introduce th...  10.804545  \n",
      "4    Dear Senator Casey,\\n  I am writing to urge yo...  10.313177  \n",
      "..                                                 ...        ...  \n",
      "345  Dear Senator, please continue to oppose this b... -13.704764  \n",
      "346  Dear Senator Warren,\\n \\n Greetings. I am gues... -13.680026  \n",
      "347  Dear Senator,\\n Please do not feed into the De... -12.395852  \n",
      "348  Dear Senator;\\n \\n  Please do not consider re-... -11.963424  \n",
      "349  Please re-introduce this bill! We need more di... -11.066627  \n",
      "\n",
      "[350 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the number of top scores to extract\n",
    "top_n = 5\n",
    "\n",
    "# Extract Top `top_n` Positive and Negative Scores for Each Rotated Component\n",
    "results = []\n",
    "for component in rotated_scores_df.columns[:-1]:  # Exclude the \"letter\" column\n",
    "    # Get the top_n positive scores\n",
    "    top_positive = rotated_scores_df.nlargest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Get the top_n negative scores\n",
    "    top_negative = rotated_scores_df.nsmallest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Store the results for positive scores\n",
    "    for letter, score in top_positive.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Positive\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "    \n",
    "    # Store the results for negative scores\n",
    "    for letter, score in top_negative.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Negative\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "\n",
    "# Step 3: Save the Results to a CSV File\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\top_scores_per_rotated_component.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e749af1-6012-4b8c-86e4-eff867817a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a601d3-2202-441e-99de-642c26e51f99",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10ee4b4-a98d-43f2-ac70-289c253d8a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"V:\\20240920\\way1\\1_merged table v1.xlsx\"\n",
    "\n",
    "csv_output_file = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "npy_output_file = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=\"sheet1\")\n",
    "df['orginal_letter'] = df['letter']\n",
    "\n",
    "## Task1: replace the specific mistake characters\n",
    "#详见《 letter列文本中的乱码及处理方法.xlsx》 \n",
    "replacements = {\n",
    "            '‚Äô': \"'\",\n",
    "            '‚äô': \"'\",\n",
    "            '√©': \" \",\n",
    "            'Äì¬†': \" \",\n",
    "            '‚Äú': \"\\\"\",\n",
    "            '‚Äù': \"\\'\",\n",
    "            '‚Äî': \"--\",\n",
    "            '‚Ä¶': \" \",\n",
    "            '√≥': \"o\"\n",
    "        }\n",
    "for old, new in replacements.items():\n",
    "    df['letter'] = df['letter'].str.replace(old, new, regex=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a0fbf-2379-47b2-94b2-af9b5370cb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "huggingface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
