{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17437819-8457-415a-bc83-22817d97df20",
   "metadata": {},
   "source": [
    "# 版本变更说明\n",
    "**enbedding 段的 input_file 是《act3301_processed_data.csv》**，这导致如下影响：  \n",
    "\n",
    "- 输入文件是经 github 上《act3301_processing.py》处理《data/data.csv》得到的数据文件《results/act3301_processed_data.csv》\n",
    "- 《data/data.csv》很像是WTOS文件的CSV版本，但我不完全确定\n",
    "\n",
    "**几个重要参数**:  \n",
    "\n",
    "- input_file = r\"V:\\20240920\\github\\results\\act3301_processed_data.csv\"\n",
    "- explained_variance_ratio = 0.7  决定PCA和varimax的compents数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f673556-df4e-4f6c-ba39-5a16f8e954bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leesb\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478595c-f058-440e-92eb-14ae077e4349",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b003189c-b3ad-4449-917f-06ec53391f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Data statistics:\n",
      "Total number of texts: 8805\n",
      "Number of None values: 3219\n",
      "Number of valid texts after removing None: 5586\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leesb\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embeddings...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5586, 768)\n",
      "length of csv_output_file: 5586\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"V:\\20240920\\github\\results\\act3301_processed_data.csv\"\n",
    "\n",
    "csv_output_file = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "npy_output_file = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Remove None values and show statistics\n",
    "print(\"\\nData statistics:\")\n",
    "print(f\"Total number of texts: {len(df)}\")\n",
    "print(f\"Number of None values: {df['letter'].isnull().sum()}\")\n",
    "   \n",
    "# Remove rows with None values\n",
    "# 必须删除 'letter' 列为空的行，否则 SentenceTransformer 会报错\n",
    "df_clean = df.dropna(subset=['letter'])\n",
    "print(f\"Number of valid texts after removing None: {len(df_clean)}\")\n",
    "\n",
    "# Creating embeddings\n",
    "print(\"Creating embeddings...\")\n",
    "texts = df_clean['letter'].tolist()\n",
    "# texts = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer(r\"V:\\huggingface\\model\\sentence-transformers/all-mpnet-base-v2\") # off-line mode\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') # on-line mode\n",
    "\n",
    "embeddings = model.encode(texts, max_length=2048)\n",
    "\n",
    "# Saving embeddings  \n",
    "print(\"\\nSaving embeddings...\")\n",
    "# np_embeddings = embeddings.numpy()\n",
    "np.save(npy_output_file, embeddings)\n",
    "\n",
    "# compare length of embedding and data file \n",
    "print(\"\\n --- compare length of embedding and data file ---\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"length of csv_output_file: {len(df_clean['letter'])}\")\n",
    "\n",
    "# Save clean data for reference\n",
    "df_clean.to_csv(csv_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a6e02-ccae-43a3-b9b1-872896d50a80",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f72d6-9a45-4333-bc5d-06a4ca732e0b",
   "metadata": {},
   "source": [
    "## explain variance percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c000fc28-5ed3-47df-a8e0-75d89db77528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 34 components to explain 70.0% variance.\n",
      "Need 63 components to explain 80.0% variance.\n",
      "Need 135 components to explain 90.0% variance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def components_for_variance(embeddings_path, n):\n",
    "    embed = np.load(embeddings_path)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embed)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_embeddings)\n",
    "    \n",
    "    # Calculate cumulative variance ratio\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    num_components = np.argmax(cumulative_variance >= n) + 1  \n",
    "    \n",
    "    print(f\"Need {num_components} components to explain {n * 100}% variance.\")\n",
    "    return num_components\n",
    "\n",
    "\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "components_for_variance(embeddings_path, 0.7)\n",
    "components_for_variance(embeddings_path, 0.8)\n",
    "components_for_variance(embeddings_path, 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd9cb2-45bb-4196-9c42-e87f81e94287",
   "metadata": {},
   "source": [
    "## Themes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93677bab-48d1-4b52-b049-d42fd47df4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 34 components to explain 70.0% variance.\n",
      "\n",
      " --- need 34 components for explained variance ratio 0.7 ---\n",
      "Loading data...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5586, 768)\n",
      "length of data file: 5586\n",
      "\n",
      "--- Standardizing embeddings...\n",
      "\n",
      "--- Applying PCA with 34 components...\n",
      "\n",
      " --- Total explained variance: 0.7024\n",
      "\n",
      "--- Explained Variance Ratio per Component ---\n",
      "Component 1: 0.1112\n",
      "Component 2: 0.0743\n",
      "Component 3: 0.0731\n",
      "Component 4: 0.0517\n",
      "Component 5: 0.0388\n",
      "Component 6: 0.0344\n",
      "Component 7: 0.0289\n",
      "Component 8: 0.0241\n",
      "Component 9: 0.0211\n",
      "Component 10: 0.0188\n",
      "Component 11: 0.0175\n",
      "Component 12: 0.0157\n",
      "Component 13: 0.0147\n",
      "Component 14: 0.0144\n",
      "Component 15: 0.0128\n",
      "Component 16: 0.0121\n",
      "Component 17: 0.0117\n",
      "Component 18: 0.0111\n",
      "Component 19: 0.0098\n",
      "Component 20: 0.0097\n",
      "Component 21: 0.0093\n",
      "Component 22: 0.0089\n",
      "Component 23: 0.0086\n",
      "Component 24: 0.0081\n",
      "Component 25: 0.0079\n",
      "Component 26: 0.0072\n",
      "Component 27: 0.0071\n",
      "Component 28: 0.0065\n",
      "Component 29: 0.0063\n",
      "Component 30: 0.0057\n",
      "Component 31: 0.0057\n",
      "Component 32: 0.0053\n",
      "Component 33: 0.0052\n",
      "Component 34: 0.0049\n",
      "\n",
      "\n",
      " --- Analyzing themes in each component...\n",
      "old File 'V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# select explained variance ratio\n",
    "explained_variance_ratio = 0.7\n",
    "number_components = components_for_variance(embeddings_path, explained_variance_ratio)\n",
    "print(f\"\\n --- need {number_components} components for explained variance ratio {explained_variance_ratio} ---\")\n",
    "\n",
    "def analyze_pca_themes(embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\", \n",
    "                       data_path=r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\",\n",
    "                       n_components = 34,   \n",
    "                       n_top_texts=5):\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # compare length of embedding and data file \n",
    "    print(\"\\n --- compare length of embedding and data file ---\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"length of data file: {len(df['letter'])}\")\n",
    "    \n",
    "    # Check data length consistency\n",
    "    if embeddings.shape[0] != len(df):\n",
    "        raise ValueError(\"The number of embeddings does not match the number of texts in the data file.\")\n",
    "  \n",
    "    # Standardize embeddings\n",
    "    print(\"\\n--- Standardizing embeddings...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "   \n",
    "    # Apply PCA\n",
    "    print(f\"\\n--- Applying PCA with {n_components} components...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(scaled_embeddings)\n",
    "   \n",
    "    print(f\"\\n --- Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    # Print explained variance ratio per component\n",
    "    print(\"\\n--- Explained Variance Ratio per Component ---\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"Component {i+1}: {ratio:.4f}\")\n",
    "   \n",
    "    # Analyze themes\n",
    "    print(\"\\n\\n --- Analyzing themes in each component...\")\n",
    "    themes = {}\n",
    "    for i in range(n_components):\n",
    "        # Get component scores\n",
    "        scores = pca_result[:, i]\n",
    "        \n",
    "        # Get top and bottom texts\n",
    "        top_indices = np.argsort(scores)[-n_top_texts:]\n",
    "        bottom_indices = np.argsort(scores)[:n_top_texts]\n",
    "        \n",
    "        # Add statistics\n",
    "        themes[f'Component_{i+1}'] = {\n",
    "            'positive': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in reversed(top_indices)],\n",
    "            'negative': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in bottom_indices],\n",
    "            'score_stats': {\n",
    "                'mean': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "        }\n",
    "   \n",
    "    return pca_result, themes\n",
    "\n",
    "def save_themes_to_excel(themes, output_path):\n",
    "    \"\"\"\n",
    "    Save the thematic analysis results to an Excel file.\n",
    "    \"\"\"\n",
    "    # Step 1: Check if the file exists and delete it\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"old File '{output_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File '{output_path}' does not exist. Creating a new file.\")\n",
    "    \n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "        for component, theme_data in themes.items():\n",
    "            # Create a DataFrame for positive examples\n",
    "            positive_df = pd.DataFrame(theme_data['positive'], columns=['Text', 'Score'])\n",
    "            positive_df.to_excel(writer, sheet_name=f\"{component}_Positive\", index=False)\n",
    "            \n",
    "            # Create a DataFrame for negative examples\n",
    "            negative_df = pd.DataFrame(theme_data['negative'], columns=['Text', 'Score'])\n",
    "            negative_df.to_excel(writer, sheet_name=f\"{component}_Negative\", index=False)\n",
    "            \n",
    "            # Add statistics to the Excel file\n",
    "            stats_df = pd.DataFrame([theme_data['score_stats']])\n",
    "            stats_df.to_excel(writer, sheet_name=f\"{component}_Stats\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Apply PCA and analyze themes\n",
    "    pca_result, themes = analyze_pca_themes(n_components = number_components)\n",
    "    \n",
    "    # Save PCA results\n",
    "    np.save(r\"V:\\20240920\\theme_analysis_act3301\\pca_results.npy\", pca_result)\n",
    "    \n",
    "    # Save themes to Excel file\n",
    "    save_themes_to_excel(themes, r\"V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b976fc-8a25-4fe0-b395-3d3d75fbde62",
   "metadata": {},
   "source": [
    "# Varimax Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18eaaea2-92f8-4b09-8916-47d11dd92cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "\n",
      "---Loadings:\n",
      " [[-0.06295693 -0.02778492 -0.03001809 ... -0.02714707 -0.02243229\n",
      "  -0.02939817]\n",
      " [-0.03351093  0.00943124 -0.04230408 ...  0.00042033 -0.01056164\n",
      "  -0.01899311]\n",
      " [-0.00152524 -0.02726339  0.01039433 ...  0.01219691  0.03105194\n",
      "   0.1086866 ]\n",
      " ...\n",
      " [-0.0795264  -0.00083973 -0.00485306 ...  0.03164923 -0.05473508\n",
      "   0.01278267]\n",
      " [-0.07967228  0.02608937 -0.01103649 ... -0.0384969  -0.06098016\n",
      "  -0.00668091]\n",
      " [ 0.02355925  0.01462261  0.07652277 ... -0.02885491 -0.01479525\n",
      "   0.04674011]]\n",
      "(768, 34)\n",
      "(34, 768)\n",
      "\n",
      "---Rotated Loadings:\n",
      " [[-0.06813098  0.00841248  0.01475781 ... -0.00619753 -0.04904796\n",
      "  -0.0459817 ]\n",
      " [ 0.00508633  0.02386861 -0.04434056 ... -0.01851083 -0.02583658\n",
      "  -0.00738636]\n",
      " [-0.01041138 -0.00582279  0.04061947 ...  0.05857896 -0.01319051\n",
      "   0.02558957]\n",
      " ...\n",
      " [-0.02170559 -0.01670777  0.02782719 ...  0.04182134 -0.02308375\n",
      "   0.02068716]\n",
      " [-0.07579595  0.04139633 -0.03087546 ...  0.00898842 -0.00890442\n",
      "  -0.0338707 ]\n",
      " [ 0.02436472  0.01023614  0.00911023 ... -0.00357945  0.00228111\n",
      "   0.00969449]]\n",
      "Rotated Loadings's shape: (768, 34)\n",
      "(34, 768)\n",
      "(768, 34)\n"
     ]
    }
   ],
   "source": [
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# load data\n",
    "embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# Standardize embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# 70% varianc explanation.\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis \n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# transposes the matrix so that each row corresponds to a feature and each column corresponds to a component.\n",
    "loadings = pca.components_.T  \n",
    "print(\"\\n---Loadings:\\n\", loadings)\n",
    "print(loadings.shape)\n",
    "print(loadings.T.shape)\n",
    "\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "print(\"\\n---Rotated Loadings:\\n\", rotated_loadings)\n",
    "print(f\"Rotated Loadings's shape: {rotated_loadings.shape}\")\n",
    "print(pca.components_.shape)\n",
    "print(pca.components_.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc525d-2a69-4d70-8c92-539b38afc037",
   "metadata": {},
   "source": [
    "## get the new score after varimax rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbaea812-f6ec-48b9-8845-f331f6e2ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "Rotated Scores' shape: (5586, 34)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# Step 1: Load the Data\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "data_path = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "\n",
    "embeddings = np.load(embeddings_path)\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Standardize the Embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Step 3: Perform PCA and Varimax Rotation\n",
    "# Perform PCA to determine the number of components explaining 90% variance\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis\n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "# Fit PCA with the selected number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# Extract PCA loadings and transpose them\n",
    "loadings = pca.components_.T\n",
    "\n",
    "# Apply Varimax rotation\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "# Step 4: Project Data onto Rotated Components\n",
    "# Calculate the new scores by multiplying the standardized embeddings with the rotated loadings\n",
    "rotated_scores = np.dot(scaled_embeddings, rotated_loadings)\n",
    "\n",
    "# Step 5: Save the New Scores\n",
    "# Save the rotated scores to a new file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores.npy\"\n",
    "np.save(output_path, rotated_scores)\n",
    "\n",
    "# Print the shape of the rotated scores\n",
    "print(f\"Rotated Scores' shape: {rotated_scores.shape}\")\n",
    "\n",
    "# Optional: Save the rotated scores with corresponding text data to a CSV file\n",
    "rotated_scores_df = pd.DataFrame(rotated_scores, columns=[f\"Rotated_Component_{i+1}\" for i in range(n_components)])\n",
    "rotated_scores_df[\"letter\"] = df[\"letter\"].values\n",
    "rotated_scores_df.to_csv(r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830b8fd-bbc5-441d-be46-9ab2e3bcc4ad",
   "metadata": {},
   "source": [
    "## get top_n positive and negative score letter of each Rotated Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd44a27a-f182-4038-b2a8-dc34b0351c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Component      Type  \\\n",
      "0     Rotated_Component_1  Positive   \n",
      "1     Rotated_Component_1  Positive   \n",
      "2     Rotated_Component_1  Positive   \n",
      "3     Rotated_Component_1  Positive   \n",
      "4     Rotated_Component_1  Positive   \n",
      "..                    ...       ...   \n",
      "335  Rotated_Component_34  Negative   \n",
      "336  Rotated_Component_34  Negative   \n",
      "337  Rotated_Component_34  Negative   \n",
      "338  Rotated_Component_34  Negative   \n",
      "339  Rotated_Component_34  Negative   \n",
      "\n",
      "                                                Letter      Score  \n",
      "0    please do not reintroduce act3301. i am oppose...  15.660507  \n",
      "1    don't reintroduce act3301. we can do better. w...  15.077238  \n",
      "2    dear senator,\\n \\n i am writing to encourage y...  15.054308  \n",
      "3    i do not favor reintroduction of act3301. it i...  14.155356  \n",
      "4    please do not reintroduce act3301. our state a...  13.906468  \n",
      "..                                                 ...        ...  \n",
      "335  dear senators ossoff and warnock:\\n \\n i have ... -10.607027  \n",
      "336  dear senators,\\n i am facing the problem pleas...  -9.856891  \n",
      "337  an important aspect of the u.s. government pro...  -9.611132  \n",
      "338                  hi senator keep up the great work  -9.442378  \n",
      "339  hello senator,\\n \\n i hope you're doing very w...  -9.245305  \n",
      "\n",
      "[340 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the number of top scores to extract\n",
    "top_n = 5\n",
    "\n",
    "# Extract Top `top_n` Positive and Negative Scores for Each Rotated Component\n",
    "results = []\n",
    "for component in rotated_scores_df.columns[:-1]:  # Exclude the \"letter\" column\n",
    "    # Get the top_n positive scores\n",
    "    top_positive = rotated_scores_df.nlargest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Get the top_n negative scores\n",
    "    top_negative = rotated_scores_df.nsmallest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Store the results for positive scores\n",
    "    for letter, score in top_positive.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Positive\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "    \n",
    "    # Store the results for negative scores\n",
    "    for letter, score in top_negative.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Negative\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "\n",
    "# Step 3: Save the Results to a CSV File\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\top_scores_per_rotated_component.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e749af1-6012-4b8c-86e4-eff867817a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
