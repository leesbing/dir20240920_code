{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17437819-8457-415a-bc83-22817d97df20",
   "metadata": {},
   "source": [
    "# note\n",
    "- The input file of enbedding cell is act3301_processed_data.csv  \n",
    "- parameter **explained_variance_ratio** decide how many components to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f673556-df4e-4f6c-ba39-5a16f8e954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478595c-f058-440e-92eb-14ae077e4349",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b003189c-b3ad-4449-917f-06ec53391f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Data statistics:\n",
      "Total number of texts: 8805\n",
      "Number of None values: 3219\n",
      "Number of valid texts after removing None: 5586\n",
      "Creating embeddings...\n",
      "\n",
      "Saving embeddings...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5586, 768)\n",
      "length of csv_output_file: 5586\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"V:\\20240920\\github\\results\\act3301_processed_data.csv\"\n",
    "\n",
    "csv_output_file = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "npy_output_file = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Remove None values and show statistics\n",
    "print(\"\\nData statistics:\")\n",
    "print(f\"Total number of texts: {len(df)}\")\n",
    "print(f\"Number of None values: {df['letter'].isnull().sum()}\")\n",
    "   \n",
    "# Remove rows with None values\n",
    "df_clean = df.dropna(subset=['letter'])\n",
    "print(f\"Number of valid texts after removing None: {len(df_clean)}\")\n",
    "\n",
    "# Creating embeddings\n",
    "print(\"Creating embeddings...\")\n",
    "texts = df_clean['letter'].tolist()\n",
    "# texts = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer(r\"V:\\huggingface\\model\\sentence-transformers/all-mpnet-base-v2\") # off-line mode\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') # on-line mode\n",
    "\n",
    "embeddings = model.encode(texts, max_length=2048)\n",
    "\n",
    "# Saving embeddings  \n",
    "print(\"\\nSaving embeddings...\")\n",
    "# np_embeddings = embeddings.numpy()\n",
    "np.save(npy_output_file, embeddings)\n",
    "\n",
    "# compare length of embedding and data file \n",
    "print(\"\\n --- compare length of embedding and data file ---\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"length of csv_output_file: {len(df_clean['letter'])}\")\n",
    "\n",
    "# Save clean data for reference\n",
    "df_clean.to_csv(csv_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a6e02-ccae-43a3-b9b1-872896d50a80",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f72d6-9a45-4333-bc5d-06a4ca732e0b",
   "metadata": {},
   "source": [
    "## explain variance percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c000fc28-5ed3-47df-a8e0-75d89db77528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 34 components to explain 70.0% variance.\n",
      "Need 63 components to explain 80.0% variance.\n",
      "Need 135 components to explain 90.0% variance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def components_for_variance(embeddings_path, n):\n",
    "    embed = np.load(embeddings_path)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embed)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_embeddings)\n",
    "    \n",
    "    # Calculate cumulative variance ratio\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    num_components = np.argmax(cumulative_variance >= n) + 1  \n",
    "    \n",
    "    print(f\"Need {num_components} components to explain {n * 100}% variance.\")\n",
    "    return num_components\n",
    "\n",
    "\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "\n",
    "components_for_variance(embeddings_path, 0.7)\n",
    "components_for_variance(embeddings_path, 0.8)\n",
    "components_for_variance(embeddings_path, 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd9cb2-45bb-4196-9c42-e87f81e94287",
   "metadata": {},
   "source": [
    "## Themes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93677bab-48d1-4b52-b049-d42fd47df4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need 34 components to explain 70.0% variance.\n",
      "\n",
      " --- need 34 components for explained variance ratio 0.7 ---\n",
      "Loading data...\n",
      "\n",
      " --- compare length of embedding and data file ---\n",
      "Embeddings shape: (5586, 768)\n",
      "length of data file: 5586\n",
      "\n",
      "--- Standardizing embeddings...\n",
      "\n",
      "--- Applying PCA with 34 components...\n",
      "\n",
      " --- Total explained variance: 0.7025\n",
      "\n",
      "--- Explained Variance Ratio per Component ---\n",
      "Component 1: 0.1112\n",
      "Component 2: 0.0743\n",
      "Component 3: 0.0731\n",
      "Component 4: 0.0517\n",
      "Component 5: 0.0388\n",
      "Component 6: 0.0344\n",
      "Component 7: 0.0289\n",
      "Component 8: 0.0241\n",
      "Component 9: 0.0211\n",
      "Component 10: 0.0188\n",
      "Component 11: 0.0175\n",
      "Component 12: 0.0157\n",
      "Component 13: 0.0147\n",
      "Component 14: 0.0144\n",
      "Component 15: 0.0128\n",
      "Component 16: 0.0121\n",
      "Component 17: 0.0117\n",
      "Component 18: 0.0111\n",
      "Component 19: 0.0098\n",
      "Component 20: 0.0097\n",
      "Component 21: 0.0093\n",
      "Component 22: 0.0089\n",
      "Component 23: 0.0086\n",
      "Component 24: 0.0081\n",
      "Component 25: 0.0079\n",
      "Component 26: 0.0072\n",
      "Component 27: 0.0071\n",
      "Component 28: 0.0065\n",
      "Component 29: 0.0063\n",
      "Component 30: 0.0057\n",
      "Component 31: 0.0057\n",
      "Component 32: 0.0053\n",
      "Component 33: 0.0052\n",
      "Component 34: 0.0050\n",
      "\n",
      "\n",
      " --- Analyzing themes in each component...\n",
      "File 'V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx' does not exist. Creating a new file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# select explained variance ratio\n",
    "explained_variance_ratio = 0.7\n",
    "number_components = components_for_variance(embeddings_path, explained_variance_ratio)\n",
    "print(f\"\\n --- need {number_components} components for explained variance ratio {explained_variance_ratio} ---\")\n",
    "\n",
    "# define function analyze_pca_themes\n",
    "def analyze_pca_themes(embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\", \n",
    "                       data_path=r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\",\n",
    "                       n_components = 34,   \n",
    "                       n_top_texts=5):\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # compare length of embedding and data file \n",
    "    print(\"\\n --- compare length of embedding and data file ---\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"length of data file: {len(df['letter'])}\")\n",
    "    \n",
    "    # Check data length consistency\n",
    "    if embeddings.shape[0] != len(df):\n",
    "        raise ValueError(\"The number of embeddings does not match the number of texts in the data file.\")\n",
    "  \n",
    "    # Standardize embeddings\n",
    "    print(\"\\n--- Standardizing embeddings...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "   \n",
    "    # Apply PCA\n",
    "    print(f\"\\n--- Applying PCA with {n_components} components...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(scaled_embeddings)\n",
    "   \n",
    "    print(f\"\\n --- Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    # Print explained variance ratio per component\n",
    "    print(\"\\n--- Explained Variance Ratio per Component ---\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"Component {i+1}: {ratio:.4f}\")\n",
    "   \n",
    "    # Analyze themes\n",
    "    print(\"\\n\\n --- Analyzing themes in each component...\")\n",
    "    themes = {}\n",
    "    for i in range(n_components):\n",
    "        # Get component scores\n",
    "        scores = pca_result[:, i]\n",
    "        \n",
    "        # Get top and bottom texts\n",
    "        top_indices = np.argsort(scores)[-n_top_texts:]\n",
    "        bottom_indices = np.argsort(scores)[:n_top_texts]\n",
    "        \n",
    "        # Add statistics\n",
    "        themes[f'Component_{i+1}'] = {\n",
    "            'positive': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in reversed(top_indices)],\n",
    "            'negative': [(df['letter'].iloc[idx], scores[idx]) \n",
    "                         for idx in bottom_indices],\n",
    "            'score_stats': {\n",
    "                'mean': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "        }\n",
    "   \n",
    "    return pca_result, themes\n",
    "\n",
    "def save_themes_to_excel(themes, output_path):\n",
    "    \"\"\"\n",
    "    Save the thematic analysis results to an Excel file.\n",
    "    \"\"\"\n",
    "    # Step 1: Check if the file exists and delete it\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"old File '{output_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File '{output_path}' does not exist. Creating a new file.\")\n",
    "    \n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "        for component, theme_data in themes.items():\n",
    "            # Create a DataFrame for positive examples\n",
    "            positive_df = pd.DataFrame(theme_data['positive'], columns=['Text', 'Score'])\n",
    "            positive_df.to_excel(writer, sheet_name=f\"{component}_Positive\", index=False)\n",
    "            \n",
    "            # Create a DataFrame for negative examples\n",
    "            negative_df = pd.DataFrame(theme_data['negative'], columns=['Text', 'Score'])\n",
    "            negative_df.to_excel(writer, sheet_name=f\"{component}_Negative\", index=False)\n",
    "            \n",
    "            # Add statistics to the Excel file\n",
    "            stats_df = pd.DataFrame([theme_data['score_stats']])\n",
    "            stats_df.to_excel(writer, sheet_name=f\"{component}_Stats\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Apply PCA and analyze themes\n",
    "    pca_result, themes = analyze_pca_themes(n_components = number_components)\n",
    "    \n",
    "    # Save PCA results\n",
    "    np.save(r\"V:\\20240920\\theme_analysis_act3301\\pca_results.npy\", pca_result)\n",
    "    \n",
    "    # Save themes to Excel file\n",
    "    save_themes_to_excel(themes, r\"V:\\20240920\\theme_analysis_act3301\\thematic_analysis_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b976fc-8a25-4fe0-b395-3d3d75fbde62",
   "metadata": {},
   "source": [
    "# Varimax Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18eaaea2-92f8-4b09-8916-47d11dd92cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "\n",
      "---Loadings:\n",
      " [[-0.0629569  -0.02778213 -0.0300207  ... -0.02277198 -0.02369534\n",
      "  -0.02629992]\n",
      " [-0.03351099  0.00943519 -0.04230314 ... -0.00102278 -0.01460425\n",
      "  -0.02007923]\n",
      " [-0.00152517 -0.02726444  0.01039183 ...  0.01405986  0.03401203\n",
      "   0.10734208]\n",
      " ...\n",
      " [-0.07952639 -0.00083943 -0.00485312 ...  0.02925705 -0.05475898\n",
      "   0.01240202]\n",
      " [-0.07967237  0.0260902  -0.01103387 ... -0.04205215 -0.06119183\n",
      "  -0.00925014]\n",
      " [ 0.02355927  0.01461541  0.07652417 ... -0.02912033 -0.00964162\n",
      "   0.04764943]]\n",
      "(768, 34)\n",
      "(34, 768)\n",
      "\n",
      "---Rotated Loadings:\n",
      " [[-0.07914463 -0.01695378  0.00733535 ...  0.00404747 -0.05561031\n",
      "   0.01636958]\n",
      " [-0.00332911  0.04855251 -0.01596029 ... -0.00571843 -0.04996122\n",
      "  -0.03579001]\n",
      " [-0.00214464 -0.0271831  -0.01474453 ...  0.01297485 -0.02828156\n",
      "   0.00481755]\n",
      " ...\n",
      " [-0.04120945 -0.01330149  0.0123803  ...  0.06255983 -0.01065167\n",
      "  -0.00270828]\n",
      " [-0.05680602  0.02780359 -0.02851707 ...  0.01432151  0.00285793\n",
      "  -0.04501653]\n",
      " [ 0.0272041   0.00713319  0.00285543 ... -0.00611648  0.01105622\n",
      "   0.05474907]]\n",
      "Rotated Loadings's shape: (768, 34)\n",
      "(34, 768)\n",
      "(768, 34)\n"
     ]
    }
   ],
   "source": [
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# load data\n",
    "embeddings_path=r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# Standardize embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# percentage of varianc explanation is set by parameter explained_variance_ratio\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis \n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# transposes the matrix so that each row corresponds to a feature and each column corresponds to a component.\n",
    "loadings = pca.components_.T  \n",
    "print(\"\\n---Loadings:\\n\", loadings)\n",
    "print(loadings.shape)\n",
    "print(loadings.T.shape)\n",
    "\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "print(\"\\n---Rotated Loadings:\\n\", rotated_loadings)\n",
    "print(f\"Rotated Loadings's shape: {rotated_loadings.shape}\")\n",
    "print(pca.components_.shape)\n",
    "print(pca.components_.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc525d-2a69-4d70-8c92-539b38afc037",
   "metadata": {},
   "source": [
    "## get the new score after varimax rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbaea812-f6ec-48b9-8845-f331f6e2ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings...\n",
      "Rotated Scores' shape: (5586, 34)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# Step 1: Load the Data\n",
    "embeddings_path = r\"V:\\20240920\\theme_analysis_act3301\\text_embeddings_clean_lb2.npy\"\n",
    "data_path = r\"V:\\20240920\\theme_analysis_act3301\\act3301_processed_data_clean.csv\"\n",
    "\n",
    "embeddings = np.load(embeddings_path)\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Standardize the Embeddings\n",
    "print(\"Standardizing embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Step 3: Perform PCA and Varimax Rotation\n",
    "# Perform PCA to determine the number of components explaining 90% variance\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "threshold = explained_variance_ratio         # keep same as themes analysis\n",
    "n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "# Fit PCA with the selected number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_embeddings)\n",
    "\n",
    "# Extract PCA loadings and transpose them\n",
    "loadings = pca.components_.T\n",
    "\n",
    "# Apply Varimax rotation\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_loadings = rotator.fit_transform(loadings)\n",
    "\n",
    "# Step 4: Project Data onto Rotated Components\n",
    "# Calculate the new scores by multiplying the standardized embeddings with the rotated loadings\n",
    "rotated_scores = np.dot(scaled_embeddings, rotated_loadings)\n",
    "\n",
    "# Step 5: Save the New Scores\n",
    "# Save the rotated scores to a new file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores.npy\"\n",
    "np.save(output_path, rotated_scores)\n",
    "\n",
    "# Print the shape of the rotated scores\n",
    "print(f\"Rotated Scores' shape: {rotated_scores.shape}\")\n",
    "\n",
    "# Optional: Save the rotated scores with corresponding text data to a CSV file\n",
    "rotated_scores_df = pd.DataFrame(rotated_scores, columns=[f\"Rotated_Component_{i+1}\" for i in range(n_components)])\n",
    "rotated_scores_df[\"letter\"] = df[\"letter\"].values\n",
    "rotated_scores_df.to_csv(r\"V:\\20240920\\theme_analysis_act3301\\rotated_scores_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830b8fd-bbc5-441d-be46-9ab2e3bcc4ad",
   "metadata": {},
   "source": [
    "## get top_n positive and negative score letter of each Rotated Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd44a27a-f182-4038-b2a8-dc34b0351c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Component      Type  \\\n",
      "0     Rotated_Component_1  Positive   \n",
      "1     Rotated_Component_1  Positive   \n",
      "2     Rotated_Component_1  Positive   \n",
      "3     Rotated_Component_1  Positive   \n",
      "4     Rotated_Component_1  Positive   \n",
      "..                    ...       ...   \n",
      "335  Rotated_Component_34  Negative   \n",
      "336  Rotated_Component_34  Negative   \n",
      "337  Rotated_Component_34  Negative   \n",
      "338  Rotated_Component_34  Negative   \n",
      "339  Rotated_Component_34  Negative   \n",
      "\n",
      "                                                Letter      Score  \n",
      "0    dear senator,\\n \\n i am writing to encourage y...  16.019330  \n",
      "1    please do not reintroduce act3301. i am oppose...  15.874529  \n",
      "2    don't reintroduce act3301. we can do better. w...  15.294627  \n",
      "3    please do not re-introduce act3301. we are alr...  14.988981  \n",
      "4    dear senator,\\n \\n the act3301 is a vital act3...  14.677087  \n",
      "..                                                 ...        ...  \n",
      "335  women entrepreneurs also face major obstacles ... -22.094580  \n",
      "336  diversity is what the economy needs to thrive.... -16.977034  \n",
      "337  women entrepreneurs also face major barriers t... -16.928652  \n",
      "338  more government isn't going to get more women ... -16.818581  \n",
      "339  it is important to continue to decrease the ge... -15.696854  \n",
      "\n",
      "[340 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the number of top scores to extract\n",
    "top_n = 5\n",
    "\n",
    "# Extract Top `top_n` Positive and Negative Scores for Each Rotated Component\n",
    "results = []\n",
    "for component in rotated_scores_df.columns[:-1]:  # Exclude the \"letter\" column\n",
    "    # Get the top_n positive scores\n",
    "    top_positive = rotated_scores_df.nlargest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Get the top_n negative scores\n",
    "    top_negative = rotated_scores_df.nsmallest(top_n, component)[[\"letter\", component]]\n",
    "    \n",
    "    # Store the results for positive scores\n",
    "    for letter, score in top_positive.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Positive\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "    \n",
    "    # Store the results for negative scores\n",
    "    for letter, score in top_negative.values:\n",
    "        results.append({\n",
    "            \"Component\": component,\n",
    "            \"Type\": \"Negative\",\n",
    "            \"Letter\": letter,\n",
    "            \"Score\": score\n",
    "        })\n",
    "\n",
    "# Step 3: Save the Results to a CSV File\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = r\"V:\\20240920\\theme_analysis_act3301\\top_scores_per_rotated_component.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e749af1-6012-4b8c-86e4-eff867817a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
